---
apiVersion: v1
kind: Namespace
metadata:
  name: llm-inferencing

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: python-inference
  namespace: llm-inferencing
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: python-inference
  template:
    metadata:
      labels:
        app: python-inference
    spec:
      containers:
      - name: python-inference
        image: thomasvn/python-inference:latest
        volumeMounts:
        - mountPath: /app/data
          name: app-storage
        resources:
          requests:
            ## Medium ##
            # cpu: 4
            # memory: 40Gi
            ## Large ##
            cpu: "24"
            memory: "177Gi"
          limits:
            nvidia.com/gpu: "1"
            ## Medium ##
            # cpu: 4
            # memory: 40Gi
            ## Large ##
            cpu: "24"
            memory: "177Gi"
        env:
        - name: HUGGINGFACE_ACCESS_TOKEN
          value: "${HUGGINGFACE_ACCESS_TOKEN}"
        - name: HF_HOME
          value: "/app/data/.cache"
        - name: HF_HUB_VERBOSITY
          value: "info"
        - name: MODEL_NAME
          value: "openai-community/gpt2"
        - name: DEVICE
          value: "cuda"
      - name: debug-container
        image: busybox
        command: ["sh", "-c", "sleep infinity"]
        volumeMounts:
        - mountPath: /app/data
          name: app-storage
        resources:
          requests:
            cpu: 10m
            memory: 20Mi
          limits:
            cpu: 10m
            memory: 20Mi
      volumes:
      - name: app-storage
        persistentVolumeClaim:
          claimName: app-pvc

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: app-pvc
  namespace: llm-inferencing
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
